{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#1-what-is-piisa","title":"1. What is PIISA?","text":"<p>PIISA stands for a Personally Identifiable Information Standard Architecture. This open source and interoperableframework creates a standard designed to allow seamless interoperability between various PII processing frameworks. </p>"},{"location":"#2-rationale","title":"2. Rationale","text":"<p>Our mission statement stems from these facts:</p> <ul> <li> <p>Proper PII management is hard, and has many facets.</p> </li> <li> <p>There are solutions available for PII processing, both open source and    commercial</p> </li> <li> <p>We might want to combine several solutions to achieve better results, or    to adapt to specific use cases</p> </li> <li> <p>However there is no practical way of achieving such combination, or of    customizing solutions</p> </li> </ul> <p>Our approach has been let's define an architecture that decomposes the PII problem into blocks, and let's define interfaces between those blocks</p> <p>Therefore the PIISA specification, and its reference implementation, tries to follow the approach of independent components that pass data between them to compose a full solution</p>"},{"location":"#3-specification","title":"3. Specification","text":"<p>Click here for the latest specification document.</p>"},{"location":"#4-usage","title":"4. Usage","text":"<p>We are developing a reference software of this specification, delivered as a set of Python packages that implement each block in the architecture. Check the libraries and an introductory usage document to find out how it has been structured and how to use it.</p>"},{"location":"#5-who-are-we","title":"5. Who are we","text":"<p>We are a team of privacy enthusiasts who are interested in improving PII management across multiple domains.  Read our blog posts.</p> <p>Current contributors to this codebase:</p> <ul> <li>@paulovn</li> <li>@ontocord</li> <li>@omri374</li> <li>@piskvorky</li> <li>@piesauce</li> <li>@edugp</li> <li>@shamikbose</li> <li>@ianyu93</li> </ul>"},{"location":"#6-contributing","title":"6. Contributing","text":"<p>We are happy to accept contributions from anyone interested in shaping out PIISA.  To contribute:</p> <ul> <li>Make sure you have a GitHub account.</li> <li>Check if a Github issue already exists. If not, create one.</li> <li>Clearly describe the issue.</li> <li>Fork the repository on GitHub.</li> <li>If your contribution contains code, please make sure you have unit tests added.</li> <li>Optional but recommended: Run <code>flake8</code> and <code>black</code> on your code prior to publishing your pull request.</li> <li>Run all tests locally before publishing your pull request.</li> <li>Push your changes to a topic branch in your fork of the repository.</li> <li>Submit a pull request to the repository.</li> </ul>"},{"location":"#7-license","title":"7. License","text":"<ul> <li>The PIISA specification is licensed under a Creative Commons   Attribution-NoDerivatives 4.0 International License.</li> <li>The PIISA reference implementation is licensed under an Apache license</li> </ul>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#1-piisa-configurations","title":"1. PIISA configurations","text":"<p>All steps in the processing chain in a PIISA framework are meant to be configurable. Such configuration can be provided by three means:</p> <ul> <li>command-line tools can provide modifiers as arguments</li> <li>object constructors can also accept some arguments as modifiers</li> <li>configuration files can integrate most of the configuration capabilities</li> </ul> <p>Those configuration files can be written in either YAML or JSON, and have three sources:</p> <ul> <li>packages contain their own local configurations as resource files; those act    as default configurations</li> <li>package-level configuration files can be supplied at object construction time</li> <li>global configuration files (containing aggregated information for several    packages) can also be supplied at object construction time</li> </ul>"},{"location":"configuration/#2-configuration-formats","title":"2. Configuration formats","text":"<p>The base syntax for those files is either YAML or JSON. The contents of one configuration file is a dictionary, whose fields depend on the specific configuration.</p> <p>There are however, two standardized fields:</p> <ul> <li><code>format</code>: this is a compulsory field, whose value is a string that    indicates the type of configuration held by the dictionary (i.e. the    configuration section; it is typically a package + module identifier).</li> <li><code>name</code>: a string giving a name to this configuration. This is optional;    if it is not present and the configuration is loaded from a file, the    framework will automatically use the filename as configuration name.</li> </ul>"},{"location":"configuration/#21-package-level","title":"2.1. Package level","text":"<p>A package-level configuration file has the structure of a dictionary. The <code>format</code> field for such a file has the general shape  <code>piisa:config:&lt;module&gt;:&lt;section&gt;:v1</code>, where</p> <ul> <li><code>piisa:config</code> is a fixed prefix</li> <li><code>&lt;module&gt;</code> identifies which module this configuration is for</li> <li><code>&lt;section&gt;</code> identifies the section in the module that is to be configured</li> <li><code>v1</code> is a version format string.</li> </ul>"},{"location":"configuration/#22-full-file","title":"2.2. Full file","text":"<p>A global configuration file contains simply a <code>config</code> field with a list of package configurations, i.e. it is a list of dictionaries, each one with its <code>format</code> key. It carries configuration for all (or many) PIISA modules from different packages. This makes possible to encapsulate in a single file a configuration for the whole PIISA toolchain.</p> <p>A full file contains also a global <code>format</code> key, whose value is <code>piisa:config:full:v1</code></p> <p>Note that in a full configuration it is possible to have more than one configuration section with the same <code>format</code> tag; they will be combined (later fields with the same name will override/update previous fields).</p> <p>The general shape is thus as follows:</p> <pre><code> {\n   \"format\": \"piisa:config:full:v1\",\n   \"config\": [\n     {\n       \"format\": \"piisa:config:&lt;module1&gt;:&lt;name1&gt;:v1\",\n       ...config for module1/name1\n     },\n     {\n       \"format\": \"piisa:config:&lt;module1&gt;:&lt;name2&gt;:v1\",\n       ...config for module1/name2\n     },\n     {\n       \"format\": \"piisa:config:&lt;module2&gt;:&lt;name&gt;:v1\",\n       ...config for module2/name\n     }\n   ]\n }\n</code></pre>"},{"location":"configuration/#3-default-configurations","title":"3. Default configurations","text":"<p>Some examples of provided default configuration files are:</p> <ul> <li>The loader.json file in the <code>pii-preprocess</code> package maps file extensions   to file types, and for each type defines a loader to read that document type</li> <li>A placeholder.json file in the <code>pii-transform</code> package defines the dummy   substitution values for the placeholder policy.</li> <li>The pii-extract-plg-transformers plugin contains a configuration file to   define the models to load and the mappping of model entities to PIISA entities</li> <li>The pii-extract-plg-presidio plugin contains a configuration file to map   Presidio entities to PIISA entities</li> </ul>"},{"location":"configuration/#4-custom-configurations","title":"4. Custom configurations","text":"<p>The default files can be replaced at execution time by custom configurations. Additionally other aspects of the processing flow can be also modified with additional configurationss:</p> <ul> <li>A task configuration file can be used to define additional PII detection tasks,   perhaps coming from custom external code (defining the external tasks by   specifying their class paths). There is a small example available.</li> <li>A <code>plugins.json</code> file can be defined to define the plugins to load, and   provide custom arguments to the loader (by default the PIISA system loads all   the plugins it can detect)</li> </ul>"},{"location":"configuration/#5-dynamic-configurations","title":"5. Dynamic configurations","text":"<p>When using the APIs provided by the PIISA packages, many objects take a <code>config</code> argument in the constructor. This argument can be a single config element or a list of elements (which will be merged). Each config element, in turn, can be  * a path to a configuration file  * a dictionary containing a live configuration object, created on the fly</p> <p>This live, dynamic configuration is a dictionary, indexed by PIISA section:  * The key is the PIISA module package the configuration is for, as a    <code>&lt;module&gt;:&lt;name&gt;:v1</code> string (i.e. the same string as the <code>format</code> field    in configuration files, without the <code>piisa:config:</code> prefix)  * The value contains a standard configuration for that PIISA module</p> <p>This is an example, in this case containing a custom configuration for the <code>pii-transform</code> package:</p> <pre><code>\nfrom pii_data.types import PiiEnum\nfrom pii_transform.defs import FMT_CONFIG_TRANSFORM\n\nconfig = {\n    FMT_CONFIG_TRANSFORM: {\n        \"default\": \"annotate\",\n        \"policy\": {\n            PiiEnum.CREDIT_CARD.name: \"synthetic\",\n            PiiEnum.GOV_ID.name: \"label\"\n        }\n    }\n}\n</code></pre> <p>If used in an object constructor, a custom config will be merged with the default config for the module (if it exists), overriding any matching fields and adding the new ones.</p>"},{"location":"libraries/","title":"Libraries","text":""},{"location":"libraries/#1-software-architecture","title":"1. Software architecture","text":"<p>Our reference implementation for the PIISA specification has been developed as a set of Python packages:</p> <ul> <li>pii-data is the foundational library, containing base data structures. It    defines our abstraction for a document to be analyzed (Source Document) and    the elements defining a PII instance.</li> <li>pii-preprocess is the package in charge of reading different document    formats into a Source Document, the standard representation defined by     pii-data; as such it implements the Preprocess block in the PIISA    architecture</li> <li>pii-extract-base is the base package for the PII Detect block. It    provides the infrastructure needed to extract PII instances from Source    Documents. However, it does not implement any PII Detector itself, delegating    that task to external libraries (attached via plugins or configuration files)</li> <li>pii-extract-plg-regex is a pii-extract plugin that implements some PII    Detectors for a number of tasks and languages, based on regular expressions    (plus optional context validation and/or checksums)</li> <li>pii-extract-plg-transformers is a pii-extract plugin that implements some PII    Detectors by creating NER model pipelines running on the Hugging Face    Transformers library.</li> <li>pii-extract-plg-presidio is a pii-extract plugin that implements some PII    Detectors by calling the Microsoft Presidio library.</li> <li>pii-decide implements the Decision block</li> <li>pii-transform implements the PII Transform block of the architecture:    it takes a PII Collection created by pii-extract (and confirmed by     pii-decide), and replaces/modifies the PII strings in the original Source    Document.</li> <li>pii-process provides some end-to-end API objects and command-line scripts.</li> </ul> <p>For a brief initial tutorial for the packages, check out the usage document.</p>"},{"location":"libraries/#2-source-document","title":"2. Source Document","text":"<p>The abstraction we use to manage original data to be processed is called Source Document. It is a simple representation of a document that contains:</p> <ul> <li>a document header, containing some document-level metadata</li> <li>a list of document chunks, each one containing a text block extracted    from the document, plus some additional metadata</li> </ul> <p>On top of this representation a few variants have been defined to carry  some particular document structures: sequence, tree, table</p> <p>Downstream tools (such as the ones in the pii-extract-base or pii-transform packages) can iterate over Source Documents, processing them chunk by chunk (they can also process single-chunk documents, for the case in which there is no structure to be used).</p>"},{"location":"proposals/","title":"Proposals","text":""},{"location":"proposals/#1-proposed-future-enhancementsmodifications-to-the-standard","title":"1. Proposed future enhancements/modifications to the standard","text":"<p>These are ongoing conversations, which may or may not translate into future revisions of the specification:</p>"},{"location":"proposals/#2-improvements","title":"2. Improvements","text":"<ul> <li>There is the concept of Table layout which is 2D, where the \"adjacent\"   paragraphs are actually in 2 dimensions. In hypertext, adjacent documents   might be a graph. But this type of information is probably in a context of   the chunk. There is implicity adjacency information from say the sequence of   iteration itself. Or are we saying that iteration might not be guaranteed to   be linear in the structural interation case?</li> </ul> <p>We have 3 abstractions, and some documents will not feet neatly into   one of them. The tree abstraction would be the closest one as a   document. But hypertext links are defined (mostly) across documents, and   this type of context is something we haven't touched on yet,</p> <ul> <li>As context elements, before and after definitely works. But there could be   left and right as well. In anycase, linked data would be hard to capture   here and might just be supported by a user defined context json blob that   the user can pass along and use as a callback? Say, a json blob could have a<pre><code>{'version':0.1, 'type':, 'my content type', 'date': ..., \n 'content1': ...}\n</code></pre> </li> </ul> <p>Future more specific document types might have additional context elements,   apart from the \"simple\" before/after chunks. In fact the table document has   one, the \"column\" context, which kind of a \"hypertexty jump\" since it points   to the column name (in the header row)</p> <ul> <li>We don't talk about security, but one thing to note is that since we are   serializing, the data will be stored in some medium. It would be best if it   was the same medium as the original document (if from a secured database,   the serializaiton is put back to the secured database). However, if the   document was purely in memory, but the serialization puts it in disk, there   is a greater security risk.</li> </ul> <p>Security could be an aspect to be dealt with later, or in a parallel/wrapper   standard?</p> <ul> <li>It should be noted that the actual location of the serialization of the YAML   data doesn't have to be a \"file\". It could be a database, or any datastore   for storing the data securely and for later retrieval.</li> </ul>"},{"location":"proposals/#3-fixes","title":"3. Fixes","text":"<ul> <li>change the elements in the document header to be clearer:</li> <li><code>document</code> to <code>document-metadata</code> or similar</li> <li><code>collection</code> to <code>collection-metadata</code></li> </ul>"},{"location":"specs/","title":"PII Data Specification","text":""},{"location":"specs/#1-version-041","title":"1. Version 0.4.1","text":""},{"location":"specs/#2-overall-architecture","title":"2. Overall architecture","text":"<p>The general structure of a framework dealing with PII management could be visualized as the following diagram:</p> <p></p> <p>There are up to four processing blocks for such a framework:</p> <ol> <li>Preprocess: block whose mission is to read a document in an arbitrary format (a Word Document, a Web page, a PDF file, etc) and produce a normalized version, retaining only a simplified version of the high-level structure and all the text data.</li> <li>Detect: block in charge of processing input data (usually in text format) and performing detection of candidates to be assigned as PII data. This block uses as input:<ul> <li>source document: we will consider a normalized data format that conveys the raw text contents, together with some structural information. (which can provide useful hints to the PII Detection modules about the relations between text chunks)</li> <li>configuration information: specification of contextual elements affecting detection (e.g. text language, applicable countries, etc)</li> <li>component information: the set of available PII Detectors that can be used (assuming we take a modular approach, there might be a database of \"pluggable modules\" we can use for PII detection). Each Detector will define the type and parameters of PII that can detect.</li> </ul> </li> <li>Decide: block that takes a number of PII candidates, as produced by the Detection block, and consolidates that information, producing as final result the set of PII elements in the text that need to be addressed. In the process it might combine PII candidates, choose among overlapping PII candidates, reject others, etc. This block uses as input:<ul> <li>Candidate list: A list of detected PII candidates</li> <li>Configuration information, as provided by the Decide block (language, countries, etc)</li> <li>An optional purpose/application scenario, to guide the decisions</li> <li>Context information, as defined in its own configuration. This might include: requirements on PII specificity, sensitivity and scarcity, applicable regulations, etc</li> </ul> </li> <li> <p>Transform: This is the block that takes the decided PII entities, and acts upon them, depending on the intended purpose. There can be different Transformation blocks, all of them sharing the same interface but providing different outcomes. Some examples are:</p> <ul> <li>De-identification: removal of PII data from source material, possibly   replacing it with anonymous placeholders</li> <li>Pseudonymization: substitution of PII information by opaque identifiers   that preclude attribution to subjects</li> <li>Aggregation: compute aggregates of PII elements for e.g. statistical   purposes, or to identify possible inconsistencies.</li> <li>Auditing and explanation: track PII decisions and be able to explain   them.</li> <li>Anonymization: modify the text to eliminate decided PII entities. Depending on options they can be replaced by placeholders, dummy values, generated fake PII data, etc</li> </ul> </li> <li> <p>Visualize, evaluate, interpret:</p> <ul> <li>Record level: enable browsing the input data and highlight/examine PII decisions.</li> <li>Evaluation: provide the capability to assess the performance of detection and/or decision, possibly by using a ground truth evaluation dataset, and estimate precision values.</li> <li>Interpretability: Provide the capability to interpret the decision process (e.g. why a certain span was decided to be detected as PII and additional metadata on the decision)</li> <li>Analytics:  provide the capability to extract and visualize aggregated statistics on decided PII and their associated parameters</li> </ul> </li> </ol> <p>Note that the full process of performing PII recognition on documents could be considered as the combination of the steps Detect + Decide</p> <ul> <li>Detection is the first phase, in which matches are done</li> <li>Decision is the second phase that consolidates all detected PII instances</li> </ul>"},{"location":"specs/#21-description-of-possible-use-cases","title":"2.1. Description of possible use cases","text":"<p>This is a non-comprehensive enumeration of possible transformation use cases:</p> <ul> <li>PII redaction prior to ML model training (e.g., prior to training LLMs)</li> <li>Pseudonymization of clinical notes for secondary analysis</li> <li>Realtime redaction of PII from system logs</li> <li>Redact textual PII from images, forms, PDFs etc.</li> <li>PII de-identification on semi-structured data, e.g., specific areas in a JSON file, XML, free text columns in tabular data.</li> <li>Semi-automated PII removal (human in the loop)</li> <li>Classify documents based on whether they contain PII</li> </ul>"},{"location":"specs/#3-specification-interfaces","title":"3. Specification interfaces","text":"<p>The main interfaces to be specified are those that act as boundaries between architecture blocks:</p> <ul> <li>interface between preprocessing and detection</li> <li>interface between detection and decision</li> <li>interface between decision and transformation</li> </ul> <p>It might be possible to also define some interfaces internal to one block, so that the block can be decomposed into modular elements (e.g. for pluggable detectors inside the Decide block)</p>"},{"location":"specs/#4-specification-types","title":"4. Specification types","text":"<p>At any given interface, we can envision three types of specification:</p> <ol> <li>A data specification: syntax &amp; semantics of the data structures that will be sent through one of the interfaces</li> <li>A program specification: program interfaces to let components call or be called across the interfaces. This would need to fix an initial default programming language (e.g. Python) to be able to instantiate such program interface; additional languages might be defined later</li> <li>An API specification, as a programming language-independent way of interchanging data information across interfaces. This would use a definition such as an OpenAPI specification so that it can be applicable regardless of the programming language; to this aim the data specification would be instantiated into a JSON schema or similar</li> </ol> <p>Note that this is a nested structure: the Data specification is the minimum required element; on top of that we can add the Program specification (or a number of them, for different programming languages) and over it the API specification (or we could also add the API specification directly over the Data specification, and leave a program specification undefined)</p>"},{"location":"specs/#5-data-specification","title":"5. Data Specification","text":"<p>Taking the interfaces of the Detect block as a central point, we can define:</p> <ul> <li>as input, a Source Document, either generated directly or via the   Preprocessing block (note that this source document is the output data   specification for the Preprocessing block)</li> <li>as output, a PII collection, containing a description of PII   elements. A subspecification inside it is the PII Detector, describing a   block producing PII elements.</li> </ul> <p>The Preprocess block defines its output data specification (as a Source Document), but its input data specification is not standardized, and will depend on the available modules and their capabilities to process document formats.</p> <p>The Decision block uses the PII Collection data specification as both input and output (with an enlarged data specification in output, incorporating additional information resulting from the decision). Then the Transform block uses both a PII collection and a Source Document as input; its output depends on the transformation done.</p>"},{"location":"specs/#6-source-document","title":"6. Source document","text":"<p>We need to balance two conflicting requirements:</p> <ol> <li>an easy format to work with: it needs to be machine-processable but also amenable to human editing and reading</li> <li>an expressive format: able to reflect (at least to some level) the document structure, since that structure might be important to connect PII elements</li> </ol> <p>There are quite sophisticated \"Layout\" formats for text documents: Word documents (Office Open XML aka OOXML), PDF files, RTF, ODF (Open Document), etc. They are very complex, with specifications compressing many pages, since they allow the complete and precise specification of all aspects of document layout, structure and presentation. They would, of course, offer the greatest nuance in determining the relationships between the text chunks they contain[^1], but would be too difficult to handle for our purpose of consolidating a data interchange format for PII processing that has \"reasonable\" complexity. There are also image documents (PDF, PNG, JPEG, \u2026), with non-textual PII like people's faces, fingerprints, medical scans or signatures, which we do not consider at all at this point.</p> <p>Instead, we are aiming at a simpler solution. As a very minimum, a document can be considered as a collection of text chunks. How those chunks are structured is what generates the model to be represented. In general terms, we will consider three main document model types:</p> <ul> <li>a sequence model: a document is composed by a list of consecutive chunks</li> <li>a tree model: a hierarchical top-down structure relates text chunks one to another, so that chunks can be nested</li> <li>a table model: a 2-D structure (i.e. something that can be expressed as rows and columns, with the implicit assumption of some semantic linking across those rows and columns)</li> </ul> <p>There might be mixed documents, which contain e.g. both tree and table sections, or other structures.</p> <p>The general shape of a source document is then given by a document metadata header plus a set of document chunks. A chunk is intended to contain a portion of the document with self-contained data; the exact shape of a chunk depends on the document model type.</p>"},{"location":"specs/#61-document-header","title":"6.1. Document header","text":"<p>The document header contains metadata applicable to the full document. It is divided into sections, each one containing dictionary-like metadata.</p> <p>The following sections are defined:</p> <ul> <li><code>document</code>: contains relevant information (metadata) describing this    particular document; a non-exahustive list of possible fields (all of them    optional except <code>id</code>) is:<ul> <li><code>id</code>: an arbitrary string acting as an identifier for the document. If   the document belongs to a dataset, it should be unique across all   documents in the dataset.</li> <li><code>type</code>: the model type for the document (sequence, tree, table)   The default value, when the field is not present, is sequence</li> <li><code>date</code>: an ISO 8601 date defining the creation date of the document</li> <li><code>main_lang</code>: an ISO 639-1 code describing the language the document is   in.   For documents containing more than one language, it should express the   most frequent language in the document.</li> <li><code>other_lang</code>: a list of ISO 639-1 codes indicating other languages   present in the document</li> <li><code>country</code>: a list of one or more ISO 3166-1 country codes for which the   document may be applicable (this might be used to further define the   applicability of country-dependent PII elements)</li> <li><code>title</code>: a title for the document</li> </ul> </li> <li><code>dataset</code>: metadata describing the whole dataset this document belongs to    (so their fields are applicable to all documents in the dataset).</li> </ul> <p>Additional sections may be defined for specific document types.</p>"},{"location":"specs/#7-iteration-interfaces-for-source-documents","title":"7. Iteration interfaces for source documents","text":"<p>Once the Source Document has been specified, we can define program interfaces to iterate over a document, producing document chunks. Two different iteration interfaces have been defined for this data structure:</p> <ul> <li>structural iteration: this is the native iteration for the document,    which produces elements revealing its intrinsic data structure (therefore the    objects delivered in the iteration may differ across document types,    sequence, table, tree)</li> <li>full iteration (or flat iteration): this iteration interface flattens the    document structure and always delivers a linear sequence of chunks,    regardless of the document type.</li> </ul> <p>An additional difference between the two iteration modes appears in the  content of the contextual information delivered within each chunk:</p> <ul> <li>In structural iteration, the context fields for a chunk will not contain any    information regarding the rest of the document (since the structure is    already given by the iteration), only semantic information inherent to the    chunk</li> <li>A chunk context in full iteration, however, may contain context fields that    refer to other chunks or to the full document (depending on its structure).</li> </ul> <p>The following sections detail these iteration interfaces for each document type.</p>"},{"location":"specs/#71-sequence-documents","title":"7.1. Sequence documents","text":""},{"location":"specs/#711-structural-iteration","title":"7.1.1. Structural iteration","text":"<p>A sequence document is divided into independent, adjacent chunks. Each chunk could be conceptually considered as one document paragraph (though that split is not strictly defined; it might be possible for a document to contain chunks spanning more than one typographical paragraph, or to split a very long paragraph into more than one chunk).</p> <p>Structural iteration for these documents is simply a linear sequence of chunks. Each chunk is a dictionary containing up to three elements:</p> <ul> <li>id: an arbitrary string that should be unique per document. Its mission   is to make it easier later on to map detected PII instances to the chunk   they are part of</li> <li>data: a text section that contains the textual contents of the chunk. It   will be a string containing UTF-8 raw text. It can contain newlines, blank   lines or spacing (to be considered as part of the text structure), but no   formatting or layout contents (it is assumed that exact formatting &amp; layout   is lost when creating the source document for PII processing \u2013 this is a   price we pay for simplicity)</li> <li>context: an optional element that, when present, contains a dictionary   of context fields specific for the chunk, intended to provide additional   data to help in the detection and decision process. Note that the context is   not formally part of the chunk, and the possible fields present are not   specified (they depend on the document original format and on the capabilities   of the software that extracts it).</li> </ul> <p>In its simplest form, the content of one chunk is, as mentioned, a document paragraph. But exactly how those paragraphs are determined is application-dependent. And in any case joining together the data elements in all chunks should recover a text representation of the original document.</p>"},{"location":"specs/#712-full-iteration","title":"7.1.2. Full iteration","text":"<p>The full iteration for sequence documents is quite similar to the structural iteration (given that the document structure is flat). It produces a series of chunks, each one of them having up to three elements:</p> <ul> <li> <p>id: a unique string for the chunk (there is no guarantee that is the same   as the id in structural iteration)</p> </li> <li> <p>data: the same contents as in the data field in structural iteration</p> </li> <li> <p>context: an optional element that, when present, contains a dictionary   with the same fields as in structural iteration, plus up to three types   of context structure, all of them optional:</p> </li> <li>global: it will be a link/transposition of the information in the      document header, as described above</li> <li>before: the data content from the chunk preceding this one</li> <li>after: the data contents from the chunk following this one</li> </ul> <p>The addition of these three new context elements (over the possible context already present in the structural iteration) allows a module processing a full iteration to, in most cases, treat each chunk as an isolated data piece, avoiding the need to maintain and update document state as chunks are processed.</p> <p>Note that the context element of the chunk is a logical one, and need not be present in a static representation of the chunk, or when sent or streamed, since it would repeat information unnecessarily. Instead, it would be generated on the fly by an appropriate PII processing module, so that processing elements down the line have direct access to that context when processing a chunk, even when in streaming mode.</p>"},{"location":"specs/#72-tree-documents","title":"7.2. Tree documents","text":"<p>In this document type we are trying to preserve two main structural relations between text chunks:</p> <ol> <li>an \"is-contained-in\" relation: a text chunk can be considered as semantically contained within another chunk</li> <li>an \"is-next-to\" relation: a text chunk has a relation of being after or before another text chunk</li> </ol> <p>These two relationships can be nested and combined at will. They alone can be enough to describe many of the links that we could need to establish between text chunks (not all of them, but hopefully enough for PII determination).</p>"},{"location":"specs/#721-structural-iteration","title":"7.2.1. Structural iteration","text":"<ul> <li>A document is considered as a sequence of subtrees</li> <li>The tree is split by first-level branches (chapters/sections); each element   produced when iterating is either<ul> <li>an isolated top-level paragraph</li> <li>a full subtree, which contains nested elements, according to the   hierarchy. Each element (chunk) in the subtree can contain a (text)   payload, a subtree of elements, or both</li> </ul> </li> <li>Iteration produces the sequence of subtrees (a subtree is typically rendered   as a nested dictionary)</li> </ul> <p>When iterating, then:</p> <ul> <li>A document produces a sequence of chunks</li> <li>Each chunk contains a small dictionary with 2 to 4 elements:<ol> <li>id: an arbitrary string that should be unique per document (same as    for sequence documents)</li> <li>data: a text section that contains the textual contents of the    chunk (same as for sequence documents)</li> <li>context: (optional) a section that, as in sequence documents,    may contain document-specific context fields for the chunk</li> <li>chunks: (optional) if the current chunk contains subchunks below in the hierarchy, this element contains a sequence of them. This can be nested as needed.</li> </ol> </li> </ul> <p>A chunk position in the document hierarchy (its \"level\", with 1 being a top-level chunk) could be deduced unambiguously from its location in the nested sequence of chunks.</p> <p>Note that there is some inherent ambiguity when constructing a document with this model: for the same document, the decision on whether two blocks should be considered \"next-to\" or \"included-in\" is not always univocal, and in some cases the content of the text blocks is what gives the semantics away. It is hoped that these variants should not affect the result of any PII Detector processors significantly.</p> <p>This specification would be enough to roughly translate the overall structure of a Word document, a Web Page or a PDF file, assuming that structure can be mapped into this simple hierarchy (some documents are of course more complex than that). Also, a simple raw text document can be easily modeled as a single top-level text chunk.</p>"},{"location":"specs/#722-full-iteration","title":"7.2.2. Full iteration","text":"<p>The full iteration has similar mechanics as the full iteration in sequence documents:</p> <ul> <li>The document tree is traversed depth-first, and all data elements available    when traversing generate a chunk</li> <li>Iteration then produces a linear sequence of these chunks</li> <li>A chunk contents has the same elements as in sequence documents: <code>id</code>,    <code>data</code> and <code>context</code> (with no nested chunks)</li> <li>The context elements have also the same global, before and after fields    plus one additional field:<ul> <li>section: contains the text/data for the top-level chunk in the    subtree this chunk belongs to (typically it will contain the    sections/chapter title, but it depends on the stucture of the document)</li> </ul> </li> <li>Note that here before and after may refer to chunks above/below in the    tree, or in adjacent subtrees</li> </ul>"},{"location":"specs/#73-table-documents","title":"7.3. Table documents","text":"<p>A Table Source Document is one in which the structure has two dimensions, i.e. it is organized mainly as rows and columns (so it can be mapped to a table), which then contain some type of data. Its semantic premise is that there exists some kind of relationships along rows and along columns, relationships that may have implications in terms of PII detection and required processing approaches. Examples of tabular documents are Excel files, or CSV files.</p> <p>One particular feature of certain table documents is that they can be very large, since they may contain huge quantities of data; hence there should be a way in which they can be processed by chunks (i.e. as streaming objects), without the need to hold all their contents at once.</p> <p>As all source document model types, a table document is considered as a document header, plus a sequence of document chunks.</p> <p>The document header corresponds to the already defined specification, with one modification: in addition to the already defined <code>document</code> and <code>dataset</code> sections, there may also be a <code>column</code> section, providing specific information for each of the columns in the document (considered in sequential order). It could contain up to two subelements:</p> <ul> <li><code>name</code>: the column names, as a list of text strings</li> <li><code>description</code>: a description for each column, also as a list of text strings</li> </ul>"},{"location":"specs/#731-structural-iteration","title":"7.3.1. Structural iteration","text":"<ul> <li>A document is considered as a sequence of rows</li> <li>Each row contains a list of column values, each column value is a string with   the contents of the corresponding table cell</li> <li>Iteration then produces a sequence of chunks, one per document row.</li> <li>Each chunk contains the three usual fields: <code>id</code>, <code>data</code> and <code>context</code>. </li> <li><code>id</code> and <code>context</code>have the same structure and semantics as for the other   document types</li> <li>However, <code>data</code> is different: for table documents it contains not a text   field, but a list of text fields, one per column in the row. Hence the   <code>data</code> field can also be iterated upon.</li> </ul>"},{"location":"specs/#732-full-iteration","title":"7.3.2. Full iteration","text":"<p>For Table source documents in full iteration mode:</p> <ul> <li>A document is considered as a sequence of cells</li> <li>Cells are traversed in row-major order, top-to-bottom and left-to-right</li> <li>Traversing produces a linear list of document chunks</li> <li>A chunk contents has the same elements as in sequence documents: <code>id</code>,    <code>data</code> and <code>context</code></li> <li>The <code>data</code> element contains the contents of a single document cell</li> <li>The context elements have the usual global, before and after fields    plus one additional field:<ul> <li>column: contains the column name and index for the cell in this chunk</li> </ul> </li> <li>Note that here before and after refer to the previous &amp; next cell,    which for the first/last cells in a row will be in another row</li> </ul>"},{"location":"specs/#8-storage-format","title":"8. Storage format","text":"<p>As a support file format, we define YAML as the serialized representation of a Source Document. Those are easy to inspect visually and handle/edit manually, and also can be processed via automatic tools and packages. The YAML specification is actually somehow complex, but we would use only a subset of it: the part strictly needed to support the definitions above</p> <ul> <li>sequences, for the sequences of chunks</li> <li>mappings, for each chunk</li> <li>literal block scalars, to hold the text contents of each chunk</li> </ul> <p>YAML is therefore a good candidate for storage and for manual inspection or editing. For online APIs its equivalent representation as JSON might be more appropriate, though the result would be more involved, specially with the need to serialize the text chunks, including newlines and character escaping</p> <p>A serialized dump of a Source Document will contain then a YAML representation of the document, stored in a file, database or any repositiry. Its form will be a dictionary with three elements:</p> <ul> <li><code>format</code>: a string identifier that signals the file as being a PII Source    Document. Its value is <code>piisa:src-document:v1</code></li> <li><code>header</code>: the document general metadata, as explained in document header</li> <li><code>chunks</code>: a list of all the chunks generated in a structural iteration of    the document</li> </ul> <p>The use of structural iteration allows:</p> <ul> <li>the preservation of the document structure, which then can be regenerated    when the document is read from the file</li> <li>space savings by not including the context fields present only in the full    iteration</li> </ul> <p>Note that context fields that are present in the structural iteration will be dumped into the file.</p> <p>The PIISA repository contains:</p> <ul> <li>an example of such a Source Document.</li> <li>a Python module developed to read &amp; write this format, as well as produce structural &amp; full iterators from it.</li> </ul>"},{"location":"specs/#9-pii-collection","title":"9. PII Collection","text":"<p>A PII collection is the result of running a set of PII detectors on a source document. This result takes the form of a header + a list of detected PII instances.</p>"},{"location":"specs/#91-header","title":"9.1. Header","text":"<p>The header contains generic metadata that affects all the PII instances in the collection. Elements of this metadata are:</p> <ul> <li><code>date</code>: a timestamp on when the process was run</li> <li><code>format</code>: a string indicating the format the data is in. E.g. <code>pii:pii-collection:v1</code></li> <li><code>detectors</code>: a dictionary describing all detectors (i.e. subsystems or packages) employed to produce the list. Each entry has as key a <code>detectorId</code> (an arbitrary string), and as value a dictionary with fields</li> <li><code>name</code>: the name of the package</li> <li><code>version</code>: the package version</li> <li><code>source</code>: a string defining the origin of the package (e.g. a vendor or an organization name)</li> <li><code>url</code>: an address used as reference for the package (e.g. a website or a GitHub repository)</li> <li><code>method</code>: an optional string defining the process used for detection, e.g. <code>Regex</code>, <code>NerModel</code>, <code>Regex+Context</code>, <code>Checksum</code>, etc</li> </ul>"},{"location":"specs/#92-pii-instance","title":"9.2. PII instance","text":"<p>A PII instance describes one recognized PII entity. It can be considered as a dictionary containing three types of information:</p> <ul> <li>PII Description: set of fields characterizing the instance</li> <li><code>type</code>: a string denoting the broad class of PII this instance belongs to. Typically a set of PII types will be predefined so that it can be shared across systems.</li> <li><code>subtype</code>: certain PII classes may have optional subtypes, which help qualify its meaning. For instance, the <code>GOV_ID</code> type might have as subtypes \"driving license\", \"passport number\", etc</li> <li><code>value</code>: the text string from the document containing the PII Instance, as extracted by the detector</li> <li><code>lang</code>: the ISO 639-1 code of the language the document chunk (and possibly the PII instance) is in</li> <li><code>country</code>: the ISO 3166-1 code of the country that is relevant for the PII Instance, if any. E.g. a <code>CREDIT_CARD</code> number PII may have an associated country, while a <code>BITCOIN_ADDRESS</code> PII has not.</li> <li>PII Location: information used to place the PII instance inside the document it belongs to</li> <li><code>docid</code>: the id of the document (optional, used if the PII Collection refers to more than one document)</li> <li><code>chunkid</code>: the id of the document chunk the PII instance belongs to</li> <li><code>start</code>: position of the start of the PII instance inside the document chunk (measured as number of characters from the chunk start)</li> <li><code>end</code>: position corresponding to one character beyond the end of the PII instance inside the document chunk (note that if the PII instance is right at the end of the chunk, this value will point beyond the chunk). The relation end = start + length(value) always holds</li> <li>PII Detection: information characterizing the detection process (it can help later in the evaluation by the Decision module)</li> <li><code>detectorid</code>: the identifier for the detector that produced this PII instance, using the key defined in the Collection header</li> <li><code>score</code>: an optional floating point number between 0.0 and 1.0 that gives a measure of the confidence of the Detector on this PII instance. Each detector has its own way of assessing such confidence, so scores are not necessarily comparable across detectors.</li> </ul>"},{"location":"specs/#93-file-format","title":"9.3. File format","text":"<p>When formatting a PII Collection for storage or transmission, any format capable of preserving its structure can be used. For the ease of compatibility with most REST-type interfaces, two formats can be proposed:</p> <ul> <li>full format, for storage or local processing: contains the PII Collection as one single JSON object with two subobjects: <code>metadata</code> and <code>piiList</code></li> <li>streaming format: it uses NDJSON (aka JSONL) to send the data as separate, newline-delimited chunks:</li> <li>the first line contains the collection metadata, as a JSON object</li> <li>the rest of the lines contain the list of PII instances, one per line, each one containing a JSON object</li> </ul> <p>Both formats carry the exact same information; they only differ in its structure</p> <p>Two examples of the format are:</p> <ul> <li>Full format:</li> </ul> <pre><code>  {\n    \"metadata\": {\n      \"date\": \"2022-05-18T15:00:01+00\",\n      \"format\": \"pii:pii-collection:v1\",\n      \"detectors\": {\n        \"01\": {\n           \"name\": \"pii-manager\",\n           \"version\": \"0.6.0\",\n           \"source\": \"PIISA\",\n           \"url\": \"https://github.com/piisa\"\n        },\n        \"02\": {\n           \"name\": \"presidio\",\n           \"version\": \"1.2.2\",\n           \"source\": \"Microsoft\",\n           \"url\": \"https://microsoft.github.io/presidio/\"\n        }\n      }\n    }\n    \"pii_list\": [\n      {\n        \"type\": \"CREDIT_CARD\",\n        \"value\": \"4273 9666 4581 5642\",\n        \"lang\": \"en\",\n        \"chunkid\": \"36\",\n        \"pos\": 25,\n        \"detectorid\": \"01\",\n        \"score\": 1.0\n      },\n      {\n        \"type\": \"GOV_ID\",\n        \"subtype\": \"SSN\",\n        \"value\": \"536-90-4399\",\n        \"lang\": \"en\",\n        \"country\": \"us\",\n        \"chunkid\": \"12\",\n        \"pos\": 102,\n        \"detectorid\": \"02\"\n      },\n      {\n        \"type\": \"GOV_ID\",\n        \"subtype\": \"NIF\",\n        \"value\": \"34657934-Q\",\n        \"lang\": \"es\",\n        \"country\": \"es\",\n        \"chunkid\": \"1\",\n        \"pos\": 10,\n        \"detectorid\": \"02\"\n      }\n    }\n  }\n</code></pre> <ul> <li>Streaming format:</li> </ul> <p>(note that this example shows additional newlines not present in the file itself)</p> <pre><code>  {\"date\":\"2022-05-18T15:00:01+00\",\"format\":\"pii:pii-collection:c1\",\"detectors\":{\"01\":{\"name\":\"pii-manager\",\"version\":\"0.6.0\",\"source\": \"PIISA\", \"url\":\"https://github.com/piisa\"},\"02\":{\"name\":\"presidio\",\"version\": \"1.2.2\",\"source\":\"Microsoft\",\"url\":\"https://microsoft.github.io/presidio/\"}}}\n  {\"type\":\"CREDIT_CARD\",\"value\":\"4273966645815642\",\"lang\":\"en\",\"chunkid\":\"36\",\"pos\":25,\"detectorid\":\"01\",\"score\":1.0}\n  {\"type\":\"GOV_ID\",\"subtype\":\"SSN\",\"value\":\"536-90-4399\",\"lang\":\"en\",\"country\":\"us\",\"chunkid\":\"12\",\"pos\":102\",detectorid\":\"02\"}\n  {\"type\":\"GOV_ID\",\"subtype\":\"NIF\",\"value\":\"34657934-Q\",\"lang\":\"es\",\"country\":\"es\",\"chunkid\":\"1\",\"pos\":10\",detectorid\":\"02\"}\n</code></pre>"},{"location":"specs/#10-notes","title":"10. Notes","text":"<p>[^1]:    Though the document formatting options can be used in many different ways, not all of them with semantic meaning</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#1-tldr","title":"1. TL;DR","text":"<p>The fastest procedure to execute the implementation is to install all the required packages and then execute the end-to-end processing script. We will need   * An active Python virtualenv (3.8 or later)   * PyTorch installed in that virtualenv (either for CPU or GPU, depending     on hardware availability)</p> <p>Then setting up the process for English PII processing would be:</p> <pre><code>pip install wheel\npip install pii-process[transformers]\n</code></pre> <p>and then we execute:</p> <pre><code>pii-process-doc &lt;input-document&gt; &lt;output-document.yml&gt; --lang en --default-policy label\n</code></pre> <p>... where:</p> <ul> <li><code>&lt;input-document&gt;</code> is a text, Word o CSV file (the formats currently supported by    the <code>pii-preprocess</code> package), or a YAML dump of an already-parsed document.</li> <li><code>&lt;output-document.yml&gt;</code> is a YAML representation of the document with    all found PII entities changed to a label that indicates the type of PII</li> <li><code>--lang en</code> indicates the language to use (using the ISO 639-1 two-letter    code). This is required because some PII Detectors are customized per    language (but if the document metadata already contains a language tag, then    it will be used from there, and this command-line option is not needed).</li> <li><code>label</code> is the name of the policy to apply to modify the PII occurrences;    current choices are <code>passthrough</code>, <code>redact</code>, <code>hash</code>, <code>label</code>,    <code>placeholder</code>, <code>synthetic</code> or <code>annotate</code>.  Future versions might define    additional policies</li> </ul> <p>Additionally:</p> <ul> <li>An alternative script that can process JSONL multi-documents is    pii-process-jsonl, see below.</li> <li>Output document can also be a JSON or text file (just change the file    extension), or an equivalent compressed file (e.g. use a <code>name.yml.gz</code>    filename). If the input document is a table (a CSV file), the output can    also be a CSV file.</li> <li>The argument <code>--save-pii &lt;output&gt;</code> will save in a JSON file the extracted    PII entities, as a collection.</li> <li>To get a list of the currently installed capabilities in terms of PII    detection tasks, execute <code>pii-task-info list-tasks</code></li> <li>To get a list of all languages for which there is at least one available    detector task, execute <code>pii-task-info list-languages</code></li> <li>For additional languages, there may be models available to detect some of the    PII Entities. These models would need to be installed. Check the    Transformers plugin docs for installation instructions.</li> <li>In addition to the Transformers-based plugin, there is also another    available plugin for model-based PII detection: Presidio plugin, which    uses Microsoft Presidio for detection. It can be used as an alternative, or    in combination; check the [pii-process] package documentation for installation    instructions.</li> </ul>"},{"location":"usage/#multi-language-processing-for-jsonl-files","title":"Multi-language processing for JSONL files","text":"<p>There is a variant, provided by the pii-process-jsonl script. This one assumes that the format is in JSONL format (a series of lines, each one containing a full JSON document), and that each document may be in a different language. Provided the languages are supported by the packages, it can generate an output JSONL file with the desired transformations on the PII instances detected.</p>"},{"location":"usage/#2-full-process","title":"2. Full process","text":"<p>The whole workflow is structured around a set of Python libraries, which coordinate to perform the whole process. Here we comment briefly these processing stages.</p>"},{"location":"usage/#21-preprocess","title":"2.1 Preprocess","text":"<p>In order to process documents in different formats than YAML or JSON, we need the <code>pii-preprocess</code> package. This will add a <code>pii-preprocess</code> command-line script that can read documents in some other formats and convert them to YAML Source Documents, hence allowing its processing by <code>pii-detect</code>.</p> <p>The current supported formats are: plain text files (with different options on how to split the document in chunks), Microsoft Word files and CSV files. Future versions, or plugins, will add more formats.</p>"},{"location":"usage/#22-detect","title":"2.2 Detect","text":"<ul> <li>The minimum package installation requirement for PII detection is    [<code>pii-extract-base</code>] (which will also install <code>pii-data</code>). </li> <li>However this package does not contain any detectors. Installing a plugin   will include detectors. Three plugins are available:<ul> <li><code>pii-extract-plg-regex</code> will add a plugin that includes some   regex-based detectors for PII instances in several languages/countries.</li> <li><code>pii-extract-plg-transformers</code> will add a Transformers plugin, which   uses models built with the Hugging Face Transformers library to perform   PII instance dectection.</li> <li><code>pii-extract-plg-presidio</code> will add a plugin that uses Microsoft   Presidio to perform PII instance dectection. Note that Presidio needs   an NLP engine for its model-based recognizers (the default is to use   spaCy)</li> </ul> </li> </ul> <p>The base detection package installs a <code>pii-detect</code> command-line script. The script can only process documents in serialized SourceDocument format (a YAML o JSON format containing the document split in chunks). It will output a PiiCollection: a JSON file containing all PII instances detected.</p> <p>The package also installs a <code>pii-task-info</code> script that can be used to query the currently installed capabilities, in terms of locally available plugins, languages and tasks.</p>"},{"location":"usage/#23-decide","title":"2.3 Decide","text":"<p>The [<code>pii-decide</code>] takes a PiiCollection and consolidates its contents, deciding which PII instances to keep and which ones to discard.</p> <p>Right now is a very simple package that only takes care of resolving PII instance overlaps (by choosing the longest instance). Future versions will add improved capabilities.</p>"},{"location":"usage/#24-transform","title":"2.4 Transform","text":"<p>The [<code>pii-transform</code>] package can read a PiiCollection and use it to modify a SourceDocument, replacing PII occurrences with a different string, according to a set of possible substitution policies.</p>"},{"location":"usage/#25-process-wrapper","title":"2.5 Process wrapper","text":"<p>The [<code>pii-process</code>] package is a wrapper that provides both an API and comand-line scripts to carry out the full end-to-end process, calling the APIs of the other four packages as needed.</p> <p>It provides two wrapper command-line scripts (as shown in the above end-to-end section):</p> <ul> <li><code>pii-process-doc</code> works as a combined processing pipeline, including   preprocessing, detection and PII transformation of a document in a single   execution.</li> <li>pii-process-jsonl does the same, but for JSONL files</li> </ul>"},{"location":"usage/#3-programmatic-api","title":"3. Programmatic API","text":"<p>In addition to command-line operation, the packages also provide a Python API that can be used to integrate processing into other workflows. Some examples are:</p> <ul> <li>the <code>pii-preprocess</code> package contains a DocumentLoader class to read    files and convert them to Source Documents</li> <li>the <code>pii-extract-base</code> package contains a Python API for PII Detection,    at various levels of detail.</li> <li>the <code>pii-transform</code> package contains an API for PII transformation</li> <li>the <code>pii-process</code> package contains wrapper APIs for end-to-end processing,    for both single- and multi-language processing (check its api document)</li> </ul>"}]}